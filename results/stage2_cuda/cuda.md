Training tokens (CUDA): per epoch=4096, total=204800
Epoch 0, Loss: 5.168034
Epoch 1, Loss: 4.828819
Epoch 2, Loss: 4.693434
Epoch 3, Loss: 4.599409
Epoch 4, Loss: 4.438262
Epoch 5, Loss: 4.275943
Epoch 6, Loss: 4.188701
Epoch 7, Loss: 3.979571
Epoch 8, Loss: 3.850794
Epoch 9, Loss: 3.666654
Epoch 10, Loss: 3.600918
Epoch 11, Loss: 3.401670
Epoch 12, Loss: 3.263098
Epoch 13, Loss: 3.147053
Epoch 14, Loss: 2.984125
Epoch 15, Loss: 2.916426
Epoch 16, Loss: 2.756461
Epoch 17, Loss: 2.672071
Epoch 18, Loss: 2.546729
Epoch 19, Loss: 2.437326
Epoch 20, Loss: 2.355522
Epoch 21, Loss: 2.225365
Epoch 22, Loss: 2.179677
Epoch 23, Loss: 2.055092
Epoch 24, Loss: 1.972043
Epoch 25, Loss: 1.899132
Epoch 26, Loss: 1.809431
Epoch 27, Loss: 1.765366
Epoch 28, Loss: 1.681660
Epoch 29, Loss: 1.637654
Epoch 30, Loss: 1.566861
Epoch 31, Loss: 1.516590
Epoch 32, Loss: 1.480041
Epoch 33, Loss: 1.410141
Epoch 34, Loss: 1.389990
Epoch 35, Loss: 1.332015
Epoch 36, Loss: 1.294037
Epoch 37, Loss: 1.262068
Epoch 38, Loss: 1.213218
Epoch 39, Loss: 1.195222
Epoch 40, Loss: 1.151835
Epoch 41, Loss: 1.139206
Epoch 42, Loss: 1.100039
Epoch 43, Loss: 1.074328
Epoch 44, Loss: 1.060757
Epoch 45, Loss: 1.018466
Epoch 46, Loss: 1.014140
Epoch 47, Loss: 0.975913
Epoch 48, Loss: 0.951944
Epoch 49, Loss: 0.937564
Total training time: 429.44300000 seconds (7.1574 minutes)

Generated sample (256 seed tokens + 30 new tokens):
In the beginning the codebase was small but it grew rapidly. Over countless late nights we wrote tensors, layers, and optimizers. The dummy data repeated the same lines until we decided to expand it. Here is a longer narrative meant to simulate a larger corpus of text for training experiments. With more lines and more variety, the gradients have something mildly interesting to chew on. You can sprinkle in references to transformers, attention mechanisms, residual connections, and other key words. Even though this is still toy data, it is intentionally verbose and meandering. Imagine stories about compilers, mathematicians, space travel, recipes, poetry, and philosophy woven together. The important part is that there are many tokens so the dataloader can produce batches of length two hundred fifty six without immediately wrapping around. So we fill this file with an absurdly long paragraph that keeps rambling about neural networks, ancient myths, fictional dialogues, and whimsical anecdotes about debugging. We mention dragons of entropy, gardens of for loops, and seas made of JSON. The more random sentences we add the closer this feels to a corpus even if it is just generated prose. Eventually the network will see enough variety to at least have something to overfit to. And so the tale continues, line after line, enumerating dreams of AI researchers, data scientists, and engineers tinkering with an absurdly long paragraph that there are many tokens so the closer this feels to a corpus even if it is just generated prose. The dummy data

Text generation time: 8.82498852 seconds
Saved trained weights to trained_weights.bin
