Training tokens (MPI+CUDA): per epoch=4096, total=204800 (world_size=8)
Epoch 0, Loss: 5.168032
Epoch 1, Loss: 4.848721
Epoch 2, Loss: 4.733808
Epoch 3, Loss: 4.667499
Epoch 4, Loss: 4.585856
Epoch 5, Loss: 4.485087
Epoch 6, Loss: 4.339321
Epoch 7, Loss: 4.226109
Epoch 8, Loss: 4.053402
Epoch 9, Loss: 3.947483
Epoch 10, Loss: 3.812050
Epoch 11, Loss: 3.674679
Epoch 12, Loss: 3.556235
Epoch 13, Loss: 3.392360
Epoch 14, Loss: 3.283488
Epoch 15, Loss: 3.142538
Epoch 16, Loss: 3.035375
Epoch 17, Loss: 2.904566
Epoch 18, Loss: 2.775875
Epoch 19, Loss: 2.684566
Epoch 20, Loss: 2.561371
Epoch 21, Loss: 2.449701
Epoch 22, Loss: 2.361585
Epoch 23, Loss: 2.276168
Epoch 24, Loss: 2.182588
Epoch 25, Loss: 2.127309
Epoch 26, Loss: 2.021111
Epoch 27, Loss: 1.962746
Epoch 28, Loss: 1.887382
Epoch 29, Loss: 1.821456
Epoch 30, Loss: 1.745462
Epoch 31, Loss: 1.690155
Epoch 32, Loss: 1.615830
Epoch 33, Loss: 1.587385
Epoch 34, Loss: 1.521429
Epoch 35, Loss: 1.492083
Epoch 36, Loss: 1.458650
Epoch 37, Loss: 1.408189
Epoch 38, Loss: 1.376131
Epoch 39, Loss: 1.336293
Epoch 40, Loss: 1.320888
Epoch 41, Loss: 1.269017
Epoch 42, Loss: 1.257959
Epoch 43, Loss: 1.214961
Epoch 44, Loss: 1.194417
Epoch 45, Loss: 1.176377
Epoch 46, Loss: 1.134088
Epoch 47, Loss: 1.130392
Epoch 48, Loss: 1.092683
Epoch 49, Loss: 1.081243
Total training time: 226.39400000 seconds (3.7732 minutes)
Saved trained weights to trained_weights.bin

Generated sample (256 seed tokens + 30 new tokens):
In the beginning the codebase was small but it grew rapidly. Over countless late nights we wrote tensors, layers, and optimizers. The dummy data repeated the same lines until we decided to expand it. Here is a longer narrative meant to simulate a larger corpus of text for training experiments. With more lines and more variety, the gradients have something mildly interesting to chew on. You can sprinkle in references to transformers, attention mechanisms, residual connections, and other key words. Even though this is still toy data, it is intentionally verbose and meandering. Imagine stories about compilers, mathematicians, space travel, recipes, poetry, and philosophy woven together. The important part is that there are many tokens so the dataloader can produce batches of length two hundred fifty six without immediately wrapping around. So we fill this file with an absurdly long paragraph that keeps rambling about neural networks, ancient myths, fictional dialogues, and whimsical anecdotes about debugging. We mention dragons of entropy, gardens of for loops, and seas made of JSON. The more random sentences we add the closer this feels to a corpus even if it is just generated prose. Eventually the network will see enough variety to at least have something to overfit to. And so the tale continues, line after line, enumerating dreams of AI researchers, data scientists, and engineers tinkering with an absurdly long paragraph that there are many tokens so the network will see enough variety to a corpus even if it is that there are many tokens

Text generation time: 8.88565398 seconds
