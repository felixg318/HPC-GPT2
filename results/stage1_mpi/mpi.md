Training tokens (MPI): per epoch=4096, total=204800 (world_size=8)
Epoch 0, Loss: 5.201166
Epoch 1, Loss: 4.909738
Epoch 2, Loss: 4.657234
Epoch 3, Loss: 4.513676
Epoch 4, Loss: 4.372570
Epoch 5, Loss: 4.194777
Epoch 6, Loss: 4.019602
Epoch 7, Loss: 3.812284
Epoch 8, Loss: 3.634256
Epoch 9, Loss: 3.444267
Epoch 10, Loss: 3.286560
Epoch 11, Loss: 3.135570
Epoch 12, Loss: 2.969585
Epoch 13, Loss: 2.834611
Epoch 14, Loss: 2.713197
Epoch 15, Loss: 2.580026
Epoch 16, Loss: 2.452893
Epoch 17, Loss: 2.337913
Epoch 18, Loss: 2.247525
Epoch 19, Loss: 2.161558
Epoch 20, Loss: 2.067901
Epoch 21, Loss: 1.986983
Epoch 22, Loss: 1.920686
Epoch 23, Loss: 1.843655
Epoch 24, Loss: 1.770636
Epoch 25, Loss: 1.725614
Epoch 26, Loss: 1.664273
Epoch 27, Loss: 1.628996
Epoch 28, Loss: 1.559248
Epoch 29, Loss: 1.525455
Epoch 30, Loss: 1.470772
Epoch 31, Loss: 1.439264
Epoch 32, Loss: 1.399520
Epoch 33, Loss: 1.375479
Epoch 34, Loss: 1.321622
Epoch 35, Loss: 1.319988
Epoch 36, Loss: 1.286617
Epoch 37, Loss: 1.266407
Epoch 38, Loss: 1.233488
Epoch 39, Loss: 1.219159
Epoch 40, Loss: 1.210178
Epoch 41, Loss: 1.181083
Epoch 42, Loss: 1.168295
Epoch 43, Loss: 1.149404
Epoch 44, Loss: 1.140108
Epoch 45, Loss: 1.127041
Epoch 46, Loss: 1.089700
Epoch 47, Loss: 1.090754
Epoch 48, Loss: 1.075718
Epoch 49, Loss: 1.072370
Total training time: 1287.76000000 seconds (21.4627 minutes)
Saved trained weights to trained_weights.bin

Generated sample (256 seed tokens + 30 new tokens):
In the beginning the codebase was small but it grew rapidly. Over countless late nights we wrote tensors, layers, and optimizers. The dummy data repeated the same lines until we decided to expand it. Here is a longer narrative meant to simulate a larger corpus of text for training experiments. With more lines and more variety, the gradients have something mildly interesting to chew on. You can sprinkle in references to transformers, attention mechanisms, residual connections, and other key words. Even though this is still toy data, it is intentionally verbose and meandering. Imagine stories about compilers, mathematicians, space travel, recipes, poetry, and philosophy woven together. The important part is that there are many tokens so the dataloader can produce batches of length two hundred fifty six without immediately wrapping around. So we fill this file with an absurdly long paragraph that keeps rambling about neural networks, ancient myths, fictional dialogues, and whimsical anecdotes about debugging. We mention dragons of entropy, gardens of for loops, and seas made of JSON. The more random sentences we add the closer this feels to a corpus even if it is just generated prose. Eventually the network will see enough variety to at least have something to overfit to. And so the tale continues, line after line, enumerating dreams of AI researchers, data scientists, and engineers tinkering with their minimal GPT implementations. The dummy data, and seas made of entropy, and seas made of entropy, and seas made of entropy, and

Text generation time: 109.74000000 seconds
