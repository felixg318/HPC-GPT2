Training tokens (serial): per epoch=4096, total=204800
Epoch 0, Loss: 5.168034
Epoch 1, Loss: 4.835179
Epoch 2, Loss: 4.635695
Epoch 3, Loss: 4.498031
Epoch 4, Loss: 4.307395
Epoch 5, Loss: 4.116387
Epoch 6, Loss: 3.955092
Epoch 7, Loss: 3.768782
Epoch 8, Loss: 3.616539
Epoch 9, Loss: 3.425641
Epoch 10, Loss: 3.287244
Epoch 11, Loss: 3.151229
Epoch 12, Loss: 2.972546
Epoch 13, Loss: 2.845131
Epoch 14, Loss: 2.692842
Epoch 15, Loss: 2.594075
Epoch 16, Loss: 2.459776
Epoch 17, Loss: 2.358055
Epoch 18, Loss: 2.234961
Epoch 19, Loss: 2.140623
Epoch 20, Loss: 2.057503
Epoch 21, Loss: 1.959350
Epoch 22, Loss: 1.889150
Epoch 23, Loss: 1.808680
Epoch 24, Loss: 1.731022
Epoch 25, Loss: 1.663653
Epoch 26, Loss: 1.595727
Epoch 27, Loss: 1.549898
Epoch 28, Loss: 1.486133
Epoch 29, Loss: 1.449035
Epoch 30, Loss: 1.384689
Epoch 31, Loss: 1.346302
Epoch 32, Loss: 1.317822
Epoch 33, Loss: 1.268267
Epoch 34, Loss: 1.242517
Epoch 35, Loss: 1.204523
Epoch 36, Loss: 1.169466
Epoch 37, Loss: 1.147755
Epoch 38, Loss: 1.106339
Epoch 39, Loss: 1.098106
Epoch 40, Loss: 1.058793
Epoch 41, Loss: 1.052256
Epoch 42, Loss: 1.019223
Epoch 43, Loss: 0.994312
Epoch 44, Loss: 0.997309
Epoch 45, Loss: 0.956744
Epoch 46, Loss: 0.957396
Epoch 47, Loss: 0.929843
Epoch 48, Loss: 0.910516
Epoch 49, Loss: 0.908195
Total training time: 15264.36600000 seconds (254.4061 minutes)
Saved trained weights to trained_weights.bin

Generated sample (256 seed tokens + 30 new tokens):
In the beginning the codebase was small but it grew rapidly. Over countless late nights we wrote tensors, layers, and optimizers. The dummy data repeated the same lines until we decided to expand it. Here is a longer narrative meant to simulate a larger corpus of text for training experiments. With more lines and more variety, the gradients have something mildly interesting to chew on. You can sprinkle in references to transformers, attention mechanisms, residual connections, and other key words. Even though this is still toy data, it is intentionally verbose and meandering. Imagine stories about compilers, mathematicians, space travel, recipes, poetry, and philosophy woven together. The important part is that there are many tokens so the dataloader can produce batches of length two hundred fifty six without immediately wrapping around. So we fill this file with an absurdly long paragraph that keeps rambling about neural networks, ancient myths, fictional dialogues, and whimsical anecdotes about debugging. We mention dragons of entropy, gardens of for loops, and seas made of JSON. The more random sentences we add the closer this feels to a corpus even if it is just generated prose. Eventually the network will see enough variety to at least have something to overfit to. And so the tale continues, line after line, enumerating dreams of AI researchers, data scientists, and engineers tinkering with an absurdly long paragraph that there are many tokens so the closer this feels to. The dummy data scientists, and seas made of entropy, and

Text generation time: 109.27246054 seconds
