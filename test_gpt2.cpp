// test_gpt_full.cpp
#include <stdio.h>
#include <math.h>

#include "tensor.h"
#include "embedding.h"
#include "gelu.h"
#include "softmax.h"
#include "linear.h"
#include "layernorm.h"
#include "head.h"
#include "multihead_attention.h"
#include "mlp.h"
#include "block.h"
#include "cross_entropy.h"
#include "gpt.h"

#include "exported_gpt_full.h"  // generated by export_gpt_full.py

static void print_max_abs_diff(const float* a, const float* b, int n, const char* name) {
    float max_diff = 0.0f;
    for (int i = 0; i < n; ++i) {
        float d = fabsf(a[i] - b[i]);
        if (d > max_diff) max_diff = d;
    }
    printf("%s max |diff| = %e\n", name, max_diff);
}

int main() {
    // Must match the Python config
    int block_size = 8;
    int vocab_size = 50;
    int n_layer    = 2;
    int n_head     = 2;
    int n_embd     = 16;
    int head_size  = n_embd / n_head;     // 8
    int hidden_dim = 4 * n_embd;          // 64

    int B = 1;
    int T = 5;

    // 1) Init GPT
    GPT g;
    gpt_init(&g, vocab_size, block_size, n_layer, n_head, n_embd, 0.0f);

    // 2) Overwrite embeddings
    int n_wte = vocab_size * n_embd;
    for (int i = 0; i < n_wte; ++i) {
        g.wte.weight.data[i] = wte_weight_ref[i];
    }

    int n_wpe = block_size * n_embd;
    for (int i = 0; i < n_wpe; ++i) {
        g.wpe.weight.data[i] = wpe_weight_ref[i];
    }

    // 3) Overwrite per-layer parameters
    // Flatten indexing helpers
    // ln*_all_ref: (n_layer, n_embd)
    for (int l = 0; l < n_layer; ++l) {
        Block* blk = &g.blocks[l];

        for (int d = 0; d < n_embd; ++d) {
            int idx_ln = l * n_embd + d;
            blk->ln1.gamma.data[d] = ln1_gamma_all_ref[idx_ln];
            blk->ln1.beta.data[d]  = ln1_beta_all_ref[idx_ln];
            blk->ln2.gamma.data[d] = ln2_gamma_all_ref[idx_ln];
            blk->ln2.beta.data[d]  = ln2_beta_all_ref[idx_ln];
        }

        // Q/K/V: Wq_all_ref, Wk_all_ref, Wv_all_ref are (n_layer, n_head, n_embd, head_size)
        int per_head_mat = n_embd * head_size;
        for (int h = 0; h < n_head; ++h) {
            int base = ((l * n_head) + h) * per_head_mat;

            for (int i = 0; i < per_head_mat; ++i) {
                blk->mha.heads[h].query.weight.data[i] = Wq_all_ref[base + i];
                blk->mha.heads[h].key.weight.data[i]   = Wk_all_ref[base + i];
                blk->mha.heads[h].value.weight.data[i] = Wv_all_ref[base + i];
            }
        }

        // Attention proj: (n_layer, n_embd, n_embd)
        int per_proj_mat = n_embd * n_embd;
        int base_proj = l * per_proj_mat;
        for (int i = 0; i < per_proj_mat; ++i) {
            blk->mha.proj.weight.data[i] = Wproj_all_ref[base_proj + i];
        }
        for (int d = 0; d < n_embd; ++d) {
            blk->mha.proj.bias.data[d] = bproj_all_ref[l * n_embd + d];
        }

        // MLP c_fc: (n_layer, n_embd, hidden_dim)
        int per_fc_mat = n_embd * hidden_dim;
        int base_fc = l * per_fc_mat;
        for (int i = 0; i < per_fc_mat; ++i) {
            blk->mlp.c_fc.weight.data[i] = Wfc_all_ref[base_fc + i];
        }
        for (int d = 0; d < hidden_dim; ++d) {
            blk->mlp.c_fc.bias.data[d] = bfc_all_ref[l * hidden_dim + d];
        }

        // MLP c_proj: (n_layer, hidden_dim, n_embd)
        int per_proj_mlp_mat = hidden_dim * n_embd;
        int base_proj_mlp = l * per_proj_mlp_mat;
        for (int i = 0; i < per_proj_mlp_mat; ++i) {
            blk->mlp.c_proj.weight.data[i] = Wproj_mlp_all_ref[base_proj_mlp + i];
        }
        for (int d = 0; d < n_embd; ++d) {
            blk->mlp.c_proj.bias.data[d] = bproj_mlp_all_ref[l * n_embd + d];
        }
    }

    // 4) Final LayerNorm
    for (int d = 0; d < n_embd; ++d) {
        g.ln_f.gamma.data[d] = ln_f_gamma_ref[d];
        g.ln_f.beta.data[d]  = ln_f_beta_ref[d];
    }

    // 5) LM head: weight (n_embd, vocab_size)
    int n_lm = n_embd * vocab_size;
    for (int i = 0; i < n_lm; ++i) {
        g.lm_head.weight.data[i] = lm_head_weight_ref[i];
    }
    // no bias in lm_head

    // 6) Input indices (idx_ref is int[B*T])
    // We can just pass idx_ref directly into gpt_forward_with_loss.

    // 7) Forward with loss
    Tensor logits_c;
    Tensor loss_c;
    gpt_forward_with_loss(&g, idx_ref, idx_ref, B, T, &logits_c, &loss_c);

    // 8) Compare logits and loss
    int n_logits = B * T * vocab_size;
    print_max_abs_diff(logits_c.data, logits_ref, n_logits, "logits");

    float loss_diff = fabsf(loss_c.data[0] - loss_ref);
    printf("loss_c = %.9f, loss_ref = %.9f, |diff| = %e\n",
           loss_c.data[0], loss_ref, loss_diff);

    // Optionally print first few logits
    printf("First few logits_c vs logits_ref:\n");
    for (int i = 0; i < (n_logits < 10 ? n_logits : 10); ++i) {
        printf("i=%d: c=% .9f, ref=% .9f\n",
               i, logits_c.data[i], logits_ref[i]);
    }

    // Cleanup
    tensor_free(&logits_c);
    tensor_free(&loss_c);
    gpt_free(&g);

    return 0;
}
