{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971b1598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass, asdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ca2775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 16      # small for debugging\n",
    "    vocab_size: int = 100     # small vocab for debugging\n",
    "    n_layer: int = 2\n",
    "    n_head: int = 2\n",
    "    n_embd: int = 32\n",
    "    dropout: float = 0.0      # turn off dropout for determinism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d9500",
   "metadata": {},
   "source": [
    "\n",
    "### Attention building blocks (Head + MultiHeadAttention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddfe5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"One head of causal self-attention with debug storage.\n",
    "\n",
    "    Input : (B, T, C = n_embd)\n",
    "    Output: (B, T, head_size)\n",
    "    \"\"\"\n",
    "    def __init__(self, head_size: int, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.key   = nn.Linear(config.n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(config.n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(config.n_embd, head_size, bias=False)\n",
    "\n",
    "        # Causal mask for this head: (block_size, block_size)\n",
    "        mask = torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "        self.register_buffer(\"tril\", mask)\n",
    "\n",
    "        # for debugging info\n",
    "        self.debug = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, C)\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.key(x)      # (B, T, head_size)\n",
    "        q = self.query(x)    # (B, T, head_size)\n",
    "        v = self.value(x)    # (B, T, head_size)\n",
    "\n",
    "        # Attention logits: (B, T, T)\n",
    "        logits = q @ k.transpose(-2, -1) * (self.head_size ** -0.5)\n",
    "\n",
    "        # Causal masking: only attend to t' <= t\n",
    "        mask = self.tril[:T, :T] == 0\n",
    "        logits_masked = logits.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        # Softmax over last dimension\n",
    "        att = F.softmax(logits_masked, dim=-1)  # (B, T, T)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        out = att @ v                           # (B, T, head_size)\n",
    "\n",
    "        # Store debug tensors (on CPU) for later inspection / saving\n",
    "        self.debug = {\n",
    "            \"x_in\": x.detach().cpu(),               # (B, T, C)\n",
    "            \"q\": q.detach().cpu(),                  # (B, T, hs)\n",
    "            \"k\": k.detach().cpu(),                  # (B, T, hs)\n",
    "            \"v\": v.detach().cpu(),                  # (B, T, hs)\n",
    "            \"logits_raw\": logits.detach().cpu(),    # before mask\n",
    "            \"logits_masked\": logits_masked.detach().cpu(),\n",
    "            \"att\": att.detach().cpu(),              # softmax weights\n",
    "            \"out\": out.detach().cpu(),              # head output\n",
    "        }\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46a694d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0, \"n_embd must be divisible by n_head\"\n",
    "        self.n_head = config.n_head\n",
    "        head_size = config.n_embd // config.n_head\n",
    "\n",
    "        # Independent heads, each with its own K/Q/V and mask\n",
    "        self.heads = nn.ModuleList([Head(head_size, config) for _ in range(config.n_head)])\n",
    "        # Final projection back to model dimension\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Concatenate head outputs on the channel dimension\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # (B, T, n_embd)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1634dde5",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef3d2afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Feed-forward network used inside each transformer block.\"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        hidden_dim = 4 * config.n_embd  # GPT-2 uses 4x expansion\n",
    "        self.c_fc = nn.Linear(config.n_embd, hidden_dim)\n",
    "        self.c_proj = nn.Linear(hidden_dim, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c46c2a",
   "metadata": {},
   "source": [
    "### Transformer Decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c07e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer Decoder block: pre-LN + self-attention + MLP with residuals.\"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-LN + residual connection around attention\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        # Pre-LN + residual connection around MLP\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd56efb",
   "metadata": {},
   "source": [
    "### GPT model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31b51489",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"GPT-2 style language model.\"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Transformer body\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),   # token embeddings\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),   # positional embeddings\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),                     # final layer norm\n",
    "        ))\n",
    "        # Language modeling head\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # Weight tying: share token embedding weights with lm_head\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # GPT-2 style parameter initialization\n",
    "        self.apply(self._init_weights)\n",
    "        # Special scaled init for residual projections (as in GPT-2)\n",
    "        for name, p in self.named_parameters():\n",
    "            if name.endswith(\"c_proj.weight\"):\n",
    "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        idx: LongTensor of shape (B, T)\n",
    "        targets: Optional LongTensor of shape (B, T)\n",
    "        Returns:\n",
    "            logits: (B, T, vocab_size)\n",
    "            loss: scalar or None\n",
    "        \"\"\"\n",
    "        B, T = idx.size()\n",
    "        if T > self.config.block_size:\n",
    "            raise ValueError(\n",
    "                f\"Cannot forward sequence of length {T}, \"\n",
    "                f\"block size is only {self.config.block_size}\"\n",
    "            )\n",
    "\n",
    "        # Token and positional embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # (T,)\n",
    "        tok_emb = self.transformer.wte(idx)        # (B, T, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos)        # (T, n_embd)\n",
    "        x = tok_emb + pos_emb                      # (B, T, n_embd)\n",
    "\n",
    "        # Transformer blocks\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        # Final layernorm\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        # Language modeling head\n",
    "        logits = self.lm_head(x)                   # (B, T, vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1)\n",
    "            )\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1a4fb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Forward pass ===\n",
      "Input idx: tensor([[1, 2, 3, 4, 5]])\n",
      "Logits shape: torch.Size([1, 5, 50])\n",
      "Loss: 3.7472751140594482\n",
      "\n",
      "=== Debug: Block 0, Head 0 ===\n",
      "x_in shape: torch.Size([1, 5, 16])\n",
      "q shape: torch.Size([1, 5, 8])\n",
      "k shape: torch.Size([1, 5, 8])\n",
      "v shape: torch.Size([1, 5, 8])\n",
      "logits_raw shape: torch.Size([1, 5, 5])\n",
      "logits_masked shape: torch.Size([1, 5, 5])\n",
      "att (softmax) shape: torch.Size([1, 5, 5])\n",
      "out shape: torch.Size([1, 5, 8])\n",
      "\n",
      "q[0, :, :]:\n",
      " tensor([[ 0.0433, -0.1484, -0.0681,  0.1077,  0.1292, -0.0265, -0.0527,  0.0224],\n",
      "        [ 0.0530, -0.2151, -0.0018,  0.0051, -0.0335,  0.0242,  0.0804, -0.0540],\n",
      "        [ 0.0262,  0.0207,  0.0550,  0.0878,  0.0659, -0.0131, -0.0234, -0.0209],\n",
      "        [ 0.0111, -0.0153, -0.0934, -0.0422,  0.0706, -0.0545, -0.0838, -0.0405],\n",
      "        [-0.0250, -0.1596,  0.0500, -0.0421, -0.0925, -0.0238,  0.0721, -0.2070]])\n",
      "\n",
      "att[0, :, :]:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5020, 0.4980, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3339, 0.3332, 0.3329, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2493, 0.2499, 0.2508, 0.0000],\n",
      "        [0.2008, 0.1999, 0.1993, 0.2005, 0.1994]])\n",
      "\n",
      "out[0, :, :]:\n",
      " tensor([[ 0.0215,  0.1245, -0.0592,  0.0764,  0.0801, -0.0528,  0.0372,  0.0747],\n",
      "        [ 0.0111,  0.1158, -0.0078,  0.0173,  0.0154, -0.0719,  0.0286,  0.0386],\n",
      "        [ 0.0372,  0.0983, -0.0198, -0.0003, -0.0021, -0.0685,  0.0244, -0.0236],\n",
      "        [-0.0012,  0.0839,  0.0066,  0.0269,  0.0307, -0.0262,  0.0243,  0.0112],\n",
      "        [-0.0245,  0.0772,  0.0134,  0.0170,  0.0189, -0.0110,  0.0255,  0.0051]])\n",
      "\n",
      "Saved debug tensors to gpt_debug_forward.pt\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Tiny config for debugging\n",
    "config = GPTConfig(\n",
    "    block_size=8,\n",
    "    vocab_size=50,\n",
    "    n_layer=2,\n",
    "    n_head=2,\n",
    "    n_embd=16,\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "model = GPT(config)\n",
    "model.eval()  # turn off dropout etc.\n",
    "\n",
    "# Fixed tiny input\n",
    "# Shape: (B=1, T=5)\n",
    "idx = torch.tensor([[1, 2, 3, 4, 5]], dtype=torch.long)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, loss = model(idx, targets=idx)\n",
    "\n",
    "print(\"=== Forward pass ===\")\n",
    "print(\"Input idx:\", idx)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "# Access debug info for block 0, head 0\n",
    "block0 = model.transformer.h[0]\n",
    "head00 = block0.attn.heads[0]\n",
    "dbg = head00.debug\n",
    "\n",
    "print(\"\\n=== Debug: Block 0, Head 0 ===\")\n",
    "print(\"x_in shape:\", dbg[\"x_in\"].shape)\n",
    "print(\"q shape:\", dbg[\"q\"].shape)\n",
    "print(\"k shape:\", dbg[\"k\"].shape)\n",
    "print(\"v shape:\", dbg[\"v\"].shape)\n",
    "print(\"logits_raw shape:\", dbg[\"logits_raw\"].shape)\n",
    "print(\"logits_masked shape:\", dbg[\"logits_masked\"].shape)\n",
    "print(\"att (softmax) shape:\", dbg[\"att\"].shape)\n",
    "print(\"out shape:\", dbg[\"out\"].shape)\n",
    "\n",
    "# Print small slices so it doesn't spam\n",
    "print(\"\\nq[0, :, :]:\\n\", dbg[\"q\"][0])\n",
    "print(\"\\natt[0, :, :]:\\n\", dbg[\"att\"][0])\n",
    "print(\"\\nout[0, :, :]:\\n\", dbg[\"out\"][0])\n",
    "\n",
    "# Save everything you might want to mirror in C++\n",
    "torch.save({\n",
    "    \"config\": asdict(config),\n",
    "    \"idx\": idx.cpu(),\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"logits\": logits.cpu(),\n",
    "    \"loss\": loss.cpu(),\n",
    "    \"block0_head0_debug\": dbg,\n",
    "}, \"gpt_debug_forward.pt\")\n",
    "\n",
    "print(\"\\nSaved debug tensors to gpt_debug_forward.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
