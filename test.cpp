#include <stdio.h>
#include <math.h>

#include "tensor.h"
#include "layernorm.h"
#include "linear.h"
#include "gelu.h"
#include "softmax.h"
#include "head.h"
#include "multihead_attention.h"
#include "mlp.h"
#include "block.h"

// This is generated by the Python exporter
#include "exported_arrays.h"

static void print_max_abs_diff(const float* a, const float* b, int n, const char* name) {
    float max_diff = 0.0f;
    for (int i = 0; i < n; ++i) {
        float d = fabsf(a[i] - b[i]);
        if (d > max_diff) max_diff = d;
    }
    printf("%s max |diff| = %e\n", name, max_diff);
}

int main() {
    int B = 1;
    int T = 5;
    int C = 16;     // n_embd
    int n_heads = 2;
    int head_size = 8;
    int hidden_dim = 4 * C; // 64

    // 1) Init Block with dummy values
    Block blk;
    block_init(&blk, C, n_heads, 0.0f /*attn dropout*/, 0.0f /*mlp dropout*/, 1 /*causal*/);

    // 2) Overwrite LayerNorm params
    for (int i = 0; i < C; ++i) {
        blk.ln1.gamma.data[i] = ln1_gamma_ref[i];
        blk.ln1.beta.data[i]  = ln1_beta_ref[i];
        blk.ln2.gamma.data[i] = ln2_gamma_ref[i];
        blk.ln2.beta.data[i]  = ln2_beta_ref[i];
    }

    // 3) Overwrite MHA head weights
    // Head 0
    int n_w_head = C * head_size;
    for (int i = 0; i < n_w_head; ++i) {
        blk.mha.heads[0].query.weight.data[i] = Wq0_ref[i];
        blk.mha.heads[0].key.weight.data[i]   = Wk0_ref[i];
        blk.mha.heads[0].value.weight.data[i] = Wv0_ref[i];
    }
    // Head 1
    for (int i = 0; i < n_w_head; ++i) {
        blk.mha.heads[1].query.weight.data[i] = Wq1_ref[i];
        blk.mha.heads[1].key.weight.data[i]   = Wk1_ref[i];
        blk.mha.heads[1].value.weight.data[i] = Wv1_ref[i];
    }

    // 4) Overwrite MHA proj weights
    int n_w_proj = C * C;
    for (int i = 0; i < n_w_proj; ++i) {
        blk.mha.proj.weight.data[i] = Wproj_ref[i];
    }
    for (int i = 0; i < C; ++i) {
        blk.mha.proj.bias.data[i] = bproj_ref[i];
    }

    // 5) Overwrite MLP weights
    // c_fc: (C -> hidden_dim)
    int n_w_fc = C * hidden_dim;
    for (int i = 0; i < n_w_fc; ++i) {
        blk.mlp.c_fc.weight.data[i] = Wfc_ref[i];
    }
    for (int i = 0; i < hidden_dim; ++i) {
        blk.mlp.c_fc.bias.data[i] = bfc_ref[i];
    }

    // c_proj: (hidden_dim -> C)
    int n_w_proj_mlp = hidden_dim * C;
    for (int i = 0; i < n_w_proj_mlp; ++i) {
        blk.mlp.c_proj.weight.data[i] = Wproj_mlp_ref[i];
    }
    for (int i = 0; i < C; ++i) {
        blk.mlp.c_proj.bias.data[i] = bproj_mlp_ref[i];
    }

    // 6) Build input tensor for block: shape (B,T,C)
    Tensor x_in;
    int x_shape[3] = {B, T, C};
    tensor_init(&x_in, 3, x_shape);

    int n_x = B * T * C;
    for (int i = 0; i < n_x; ++i) {
        x_in.data[i] = block0_x_in_ref[i];
    }

    // 7) Forward through block
    Tensor y_c;
    block_forward(&blk, &x_in, &y_c);

    // 8) Compare with PyTorch output
    int n_y = B * T * C;
    print_max_abs_diff(y_c.data, block0_x_out_ref, n_y, "block0_out");

    printf("First few y_c vs block0_x_out_ref:\n");
    for (int i = 0; i < (n_y < 10 ? n_y : 10); ++i) {
        printf("i=%d: c=% .9f, ref=% .9f\n", i, y_c.data[i], block0_x_out_ref[i]);
    }

    // Cleanup
    tensor_free(&x_in);
    tensor_free(&y_c);
    block_free(&blk);

    return 0;
}
