#include <stdio.h>
#include <math.h>

#include "tensor.h"
#include "linear.h"
#include "softmax.h"
#include "exported_arrays.h"  // generated by export_for_c.py

static void print_max_abs_diff(const float* a, const float* b, int n, const char* name) {
    float max_diff = 0.0f;
    for (int i = 0; i < n; ++i) {
        float d = fabsf(a[i] - b[i]);
        if (d > max_diff) max_diff = d;
    }
    printf("%s max |diff| = %e\n", name, max_diff);
}

int main() {
    int B = 1;
    int T = 5;
    int C_in = 16;
    int head_size = 8;

    // 1) Build x tensor: shape (B,T,C_in)
    Tensor x;
    int x_shape[3] = {B, T, C_in};
    tensor_init(&x, 3, x_shape);

    int n_x = B * T * C_in;
    for (int i = 0; i < n_x; ++i) {
        x.data[i] = x_in_ref[i];
    }

    // 2) Build Linear layers for Q, K, V: (in_dim=16, out_dim=8, no bias)
    Linear q_layer, k_layer, v_layer;
    linear_init(&q_layer, C_in, head_size, 0);
    linear_init(&k_layer, C_in, head_size, 0);
    linear_init(&v_layer, C_in, head_size, 0);

    // Copy weights from exported arrays (already (in_dim, out_dim))
    int n_w = C_in * head_size;
    for (int i = 0; i < n_w; ++i) {
        q_layer.weight.data[i] = Wq_ref[i];
        k_layer.weight.data[i] = Wk_ref[i];
        v_layer.weight.data[i] = Wv_ref[i];
    }

    // 3) Forward: q, k, v (each (B,T,head_size))
    Tensor q_c, k_c, v_c;
    linear_forward(&q_layer, &x, &q_c);
    linear_forward(&k_layer, &x, &k_c);
    linear_forward(&v_layer, &x, &v_c);

    int n_qkv = B * T * head_size;
    print_max_abs_diff(q_c.data, q_ref, n_qkv, "q");
    print_max_abs_diff(k_c.data, k_ref, n_qkv, "k");
    print_max_abs_diff(v_c.data, v_ref, n_qkv, "v");

    // 4) Compute attention scores and softmax: att[b,t_q,t_k]
    Tensor att_logits;
    int att_shape[3] = {B, T, T};
    tensor_init(&att_logits, 3, att_shape);

    float scale = 1.0f / sqrtf((float)head_size);

    for (int b = 0; b < B; ++b) {
        for (int t_q = 0; t_q < T; ++t_q) {
            for (int t_k = 0; t_k < T; ++t_k) {
                float dot = 0.0f;
                for (int d = 0; d < head_size; ++d) {
                    float qv = tensor_get3(&q_c, b, t_q, d);
                    float kv = tensor_get3(&k_c, b, t_k, d);
                    dot += qv * kv;
                }
                float score = dot * scale;
                // causal mask: disallow t_k > t_q
                if (t_k > t_q) {
                    score = -1e30f;
                }
                tensor_set3(&att_logits, b, t_q, t_k, score);
            }
        }
    }

    // Softmax over last dimension (t_k)
    Tensor att_c;
    softmax_forward(&att_logits, &att_c);  // (B,T,T)
    tensor_free(&att_logits);

    int n_att = B * T * T;
    print_max_abs_diff(att_c.data, att_ref, n_att, "att");

    // 5) Compute out = att @ v: (B,T,head_size)
    Tensor out_c;
    int out_shape[3] = {B, T, head_size};
    tensor_init(&out_c, 3, out_shape);

    for (int b = 0; b < B; ++b) {
        for (int t_q = 0; t_q < T; ++t_q) {
            for (int d = 0; d < head_size; ++d) {
                float sum = 0.0f;
                for (int t_k = 0; t_k < T; ++t_k) {
                    float alpha = tensor_get3(&att_c, b, t_q, t_k);
                    float vv    = tensor_get3(&v_c, b, t_k, d);
                    sum += alpha * vv;
                }
                tensor_set3(&out_c, b, t_q, d, sum);
            }
        }
    }

    int n_out = B * T * head_size;
    print_max_abs_diff(out_c.data, out_ref, n_out, "out");

    // Optional: print first few out entries
    printf("First few out_c vs out_ref:\n");
    for (int i = 0; i < (n_out < 10 ? n_out : 10); ++i) {
        printf("i=%d: c=% .9f, ref=% .9f\n", i, out_c.data[i], out_ref[i]);
    }

    // Cleanup
    tensor_free(&x);
    tensor_free(&q_c);
    tensor_free(&k_c);
    tensor_free(&v_c);
    tensor_free(&att_c);
    tensor_free(&out_c);
    linear_free(&q_layer);
    linear_free(&k_layer);
    linear_free(&v_layer);

    return 0;
}
